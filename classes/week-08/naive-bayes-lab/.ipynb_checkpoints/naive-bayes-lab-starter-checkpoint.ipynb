{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Naive Bayes Language Detection Lab\n",
    "\n",
    "_Author: David Yerrington (SF) _\n",
    "\n",
    "In this lab, we will use Naive Bayes (and other classifiers) to auto-detect the language of a given tweet. We will then assess the performance of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, seaborn as sns, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"datasets/tweets_language.csv\", encoding=\"utf-8\", index_col=0)\n",
    "tweets_df.index = tweets_df.index.astype(int)    # By default, everything read in is a string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9431 entries, 0 to 9408\n",
      "Data columns (total 2 columns):\n",
      "LANG    9409 non-null object\n",
      "TEXT    9409 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 221.0+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note above that some rows are null, which we cannot use for training\n",
    "tweets_df = tweets_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LANG</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>The #Yolo bailout: Greece's ex-finance chief h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Another mental Saturday night. It will be near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>Sometimes you take bedtime selfies w yer hat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>Currently just changed my entire outfit includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>I just like listening to @SpotifyAU's top 100 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LANG                                               TEXT\n",
       "0   en  The #Yolo bailout: Greece's ex-finance chief h...\n",
       "1   en  Another mental Saturday night. It will be near...\n",
       "2   en  Sometimes you take bedtime selfies w yer hat s...\n",
       "3   en  Currently just changed my entire outfit includ...\n",
       "4   en  I just like listening to @SpotifyAU's top 100 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data exploration\n",
    "\n",
    "#### 1.A. Explore a list of tweet words that occur more than 50x\n",
    "Plot a histogram of some kind might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us\n",
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\n",
    "X_all    =  cvt.fit_transform(tweets_df['TEXT'])\n",
    "\n",
    "# Complete the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.B. Investigate histogram of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.C. Try it again with stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us\n",
    "cvt      =  CountVectorizer(strip_accents='unicode')\n",
    "X_all    =  cvt.fit_transform(insults_df['Comment'])\n",
    "\n",
    "# Complete the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.D. Explore ngrams between 2 and 4\n",
    "Display the top 75 ngrams with frequencies.  Look at each class to see how they are same / different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look up the appropriate parameters\n",
    "# CountVectorizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.E. (Optional) try expanding the list of stopwords\n",
    "There are definitely some non-words such as web urls, etc. If you could remove them, this could help us improve the score.  Identify word/tokens that don't add much value to either class.  **You should additionally look at ngrams per language to fine tune your preprocessing.  This has the greatest potential to improve your results without tuning any model parameters.**\n",
    "\n",
    "Using `nltk.corpus`, we can get a baseline list of stop words.  Try to expand it and pass it to our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup a test / train split of your data using any method you wish.\n",
    "Try 70/30 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup a \"Pipeline\" to vectorize and use MultinomialNB classifier.\n",
    "Use `lowercase`, `strip_accents`, `Pipeline`, and optionally your updated `stop_words`.  Fit your comment data using the \"Insult\" feature as your response.\n",
    "\n",
    "Fit your training data to your pipeline, then score it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-43c7c1054e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ]) \n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TEXT\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LANG\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;31m# don't forget to score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Here's the code -- you can adapt it from here on out.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', MultinomialNB())\n",
    "]) \n",
    "\n",
    "pipeline.fit(tweets_train[\"TEXT\"], tweets_train[\"LANG\"])\n",
    "\n",
    "# don't forget to score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A. Swap out MultinomialNB with BernoulliNB in the pipeline\n",
    "How do they compare? Do you have a guess of why BernoulliNB is so poor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B. Also try logistic regression and random forests in the pipeline\n",
    "How do they compare? Recall that Logistic Regression is discriminative whereas Naive Bayes is generative. Logistic Regression uses optimization to fit a formula that discriminates between the classes, whereas Naive Bayes essentially just computes aggregate statistics. So, Logistic Regression should have a longer training time than Naive Bayes -- does it here? (see `%time`)\n",
    "\n",
    "Note that Logistic Regression and Random Forests both allow you to see feature \"importance\"/coefficients. In this case, these coefficients will inform you how strong each word indicates a language. Optionally, see if you can sort these coefficients by their values to get the strongest and weakest indicator words for languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C. Also try tweaking the paramters of CountVectorizer and TfidfTranformer\n",
    "\n",
    "Remove Tfidf also. Good / bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check your score.\n",
    "Which languages does your model work best at? Run a classification report for all languages. [Plot AUC/ROC](../../week-04/2.3-evaluating_model_fit/code/AUC-ROC-codealong.ipynb) for particular languages (vs all others) and compare them -- do they show that some languages perform better? Does our model perform worse than guessing on some languages? Also, [review classfication reporting metrics](../../week-04/4.3-advanced-model_evaluation/code/starter-code/week4-4.1-classification-report.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the code to display the classification report\n",
    "print classification_report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting: ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_roc(y, probs):\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        # probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "    mean_tpr /= len(cv)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(y, probs, threshmarkers=None):\n",
    "    fpr, tpr, thresh = roc_curve(y, probs)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "   \n",
    "    plt.xlabel(\"False Positive Rate\\n(1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive Rate\\n(Sensitivity)\")\n",
    "    plt.xlim([-0.025, 1.025])\n",
    "    plt.ylim([-0.025, 1.025])\n",
    "    plt.xticks(np.linspace(0, 1, 21), rotation=45)\n",
    "    plt.yticks(np.linspace(0, 1, 21))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using your pipeline, predict the probabilities of each language\n",
    "# Then, call plot_roc\n",
    "\n",
    "## Your code here to predict the probabilities of each class\n",
    "\n",
    "# EXAMPLE of testing a particular language\n",
    "# plot_roc(tweets_test['LANG'].apply(lambda x: x == \"en\"), predicted_proba[:, list(pipeline.classes_).index(\"en\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Check out your baseline.\n",
    "\n",
    "What is the chance that you will randomly guess correctly without any modeling? Assume the language of your input phrase has the same chance of appearing as the languages in your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is your model not getting right?\n",
    "\n",
    "Check out the incorrectly classified tweets.  Any noticable patterns? Can you explain why many of these are incorrectly classified, given what you know about how Naive Bayes works?  Pay particular attention to the recall metric.  What might be done in preprocessing steps to improve accuracy?  \n",
    "\n",
    "- Try to improve with your **preprocessing first**\n",
    "- THEN try to tweak your **parameters to your model(s)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Practice\n",
    "In the datasets directory are two additional datasets you can use for additional pratice:\n",
    "\n",
    "- **/datasets/tweets_sentiment.csv** - Sentiment analysis\n",
    "\n",
    "- **/datasets/insults_train.csv** - [Kaggle dataset](https://www.kaggle.com/c/detecting-insults-in-social-commentary). _WARNING:_ This content is fairly provacative and contains very offensive and insensitive words. However, this type of problem is very common in the continuum of comment threads throughout the web.\n",
    "\n",
    "    - Check [this blog post](http://webmining.olariu.org/my-first-kaggle-competition-and-how-i-ranked/) by a guy who used SVM, a \"neural network\", a ton of cleaning, then placed 3rd in a Kaggle competition featuing this same dataset. Also see [this blog post](http://peekaboo-vision.blogspot.de/2012/09/recap-of-my-first-kaggle-competition.html) -- he got 6th place and found the best model was a simple Logistic Regression!\n",
    "\n",
    "#### Where Next?\n",
    "\n",
    "If you're interested in this type of problem, a great area to read up on is sentiment analysis.  This [Kaggle dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) is an excellent opportunity to practice more.  Also these whitepapers are great places to expand on this topic:\n",
    "\n",
    "- [Fast and accurate sentiment classification using an\n",
    "enhanced Naive Bayes model](http://arxiv.org/pdf/1305.6143.pdf) *Great overview!*\n",
    "- [Sarcasm Detection](http://www.aclweb.org/anthology/P15-2124)\n",
    "- [Making Computers Laugh:\n",
    "Investigations in Automatic Humor Recognition](http://www.aclweb.org/anthology/H05-1067)\n",
    "- [Modelling Sarcasm in Twitter, a Novel Approach](http://www.aclweb.org/anthology/W14-2609)\n",
    "- [RUNNING HEAD: NARCISSISM AND LIE DETECTION](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/107345/zarins.finalthesis.pdf?sequence=1) *The study metrics are interesting*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
