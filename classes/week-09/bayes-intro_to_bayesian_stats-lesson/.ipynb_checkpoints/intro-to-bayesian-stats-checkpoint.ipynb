{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Bayesian Statistics\n",
    "\n",
    "_Authors: Matt Brems (DC), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Review the axioms and properties of probability\n",
    "- Cover the formula for Bayes rule\n",
    "- Learn the diachronic interpretation of Bayes rule\n",
    "- Gain an intuition for the different components of the formula\n",
    "- Tackle the Monty Hall problem with Bayesian statistics\n",
    "- Complete some additional Bayesian statistics problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson Guide\n",
    "- [Review of probability](#review)\n",
    "    - [Axioms of probability](#axioms)\n",
    "    - [Properties of probability](#properties)\n",
    "- [Bayes rule](#bayes-rule)\n",
    "    - [The \"diachronic\" interpretation](#diachronic)\n",
    "- [Frequentist vs. Bayesian probability](#freq-vs-bayes)\n",
    "- [Bayes rule in parts](#parts)\n",
    "- [The Monty Hall problem](#monty-hall)\n",
    "- [Additional Bayesian statistics problems](#additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review'></a>\n",
    "## Review of probability\n",
    "\n",
    "---\n",
    "\n",
    "### The sample space, event space, and probability function\n",
    "\n",
    "With $S$ denoting the \"sample space\" and $F$ denoting the \"event space\" or space of all possible events, we have a probability function $P$ defined as:\n",
    "\n",
    "### $$ P(S, F) \\rightarrow [0, 1]$$\n",
    "\n",
    "The probability maps all events in the sample space to the interval from 0 to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='axioms'></a>\n",
    "### Axioms of probability\n",
    "\n",
    "**Nonnegativity**\n",
    "\n",
    "For any event $A$, the probability of the event must be greater than or equal to zero.\n",
    "\n",
    "### $$ 0 \\le P(A) $$\n",
    "\n",
    "**Unit measure**\n",
    "\n",
    "The probability of the entire sample space is 1.\n",
    "\n",
    "### $$ P(S) = 1 $$\n",
    "\n",
    "**Additivity**\n",
    "\n",
    "For mutually exclusive, or in other words \"disjoint\" events $E$, the probability of any of the events occuring is equivalent to the sum of their probabilties.\n",
    "\n",
    "### $$ P\\left(\\cup_{i=1}^{\\infty}\\; E_i \\right) = \\sum_{i=1}^{\\infty} P(E_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='properties'></a>\n",
    "### Properties of probability\n",
    "\n",
    "**The probability of no event**\n",
    "\n",
    "The probability of the empty set, denoted $\\emptyset$, is zero.\n",
    "\n",
    "### $$ P\\left(\\emptyset \\right) = 0 $$\n",
    "\n",
    "**The probability of A or B occuring (union)**\n",
    "\n",
    "The probability of event $A$ or event $B$ occuring is equivalent to the sum of their individual probabilities minus the intersection of their probabilities (the probability they both occur).\n",
    "\n",
    "### $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
    "\n",
    "**Conditional probability**\n",
    "\n",
    "The probability of an event conditional on another event is written using a vertical bar between the two events. The probability of event $A$ occuring _given_ event $B$ occurs is calculated:\n",
    "\n",
    "### $$ P(A | B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "Meaning the probability of both $A$ and $B$ occuring divided by the probability that $B$ occurs at all.\n",
    "\n",
    "**Joint probability**\n",
    "\n",
    "The joint probability of two events $A$ and $B$ is a reformulation of the above equation.\n",
    "\n",
    "### $$ P(A \\cap B) = P(A|B) \\; P(B) $$\n",
    "\n",
    "Verbally, if we want to know the probability that both $A$ and $B$ happen, we can multiply the probability that $B$ happens by the probability that $A$ happens given $B$ happens.\n",
    "\n",
    "**The law of total probability**\n",
    "\n",
    "Lets say we want to know the probability of the event $B$ occuring across _all_ different events $A$. For example, lets say that we are a judge presiding over a murder trial. $B$ is the event that the suspect's wallet was found at the scene of the murder. We have many hypotheses or possible scenarios in which the wallet is found at the murder scene, one of which that the suspect was actually at the scene of the crime at the time of the murder.\n",
    "\n",
    "These different events $A$, our scenarios, are disjoint. The _total probability_ of $B$ is the probability across all of these scenarios that the wallet is found at the murder scene. So in other words - regardless of which possible scenario $A$ - what is the probability overall that the wallet is at the murder scene?\n",
    "\n",
    "### $$ P(B) = \\sum_{i=1}^n P(B \\cap A_i) $$\n",
    "\n",
    "![total probability](./assets/images/output_27_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bayes-rule'></a>\n",
    "## Bayes rule\n",
    "\n",
    "---\n",
    "\n",
    "Bayes Rule relates the probability of $A$ given $B$ to the probability of $B$ given $A$. This rule is critical for performing statistical inference, as we shall see shortly. It is formulated as:\n",
    "\n",
    "### $$ P(A|B) = \\frac{P(B|A)\\;P(A)}{P(B)} $$\n",
    "\n",
    "Let's return to the courtroom example.\n",
    "\n",
    "Say $A$ is the event that the suspect is guilty.\n",
    "\n",
    "$B$ is the event that the suspect's wallet was found at the scene of the crime.\n",
    "\n",
    "Using Bayes rule, we phrase this as: the probability that the suspect is guilty given the suspect's wallet was found at the scene of the crime is equivalent to the probability that the suspect's wallet was found there given the suspect is guilty, times the probability that the suspect is guilty (without evidence) and divided by the total probability that the wallet is found at the scene of the crime.\n",
    "\n",
    "<a id='diachronic'></a>\n",
    "### The \"diachronic\" interpretation of Bayes Rule\n",
    "\n",
    "We can re-write the formula for Bayes Rule in the context of hypotheses and data, as we have already been doing with the courtroom example. The diachronic interpretation is for the probability of events _over time_. As in, the probability of an event changes over time as we collect new data.\n",
    "\n",
    "In this case we have a model or a statistic, and we are asking the probability of our model given the data that we have observed.\n",
    "\n",
    "### $$P\\left(model\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;model\\right)}{P(data)}\\; P\\left(model\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='freq-vs-bayes'></a>\n",
    "## Frequentist vs. Bayesian probability\n",
    "\n",
    "---\n",
    "\n",
    "### Frequentism\n",
    "\n",
    "Frequentists believe the \"true\" value of a statistic about a population (for example, the mean) is fixed (and not known). We can infer more more about this \"true\" distribution by engaging in sampling, testing for effects, and studying relevant parameters of the population.\n",
    "\n",
    "Say we are flipping a coin and want to know the probability of heads. Frequentists formulate the probability of heads as a limit, defining the true probability of heads derived from an infinite number of coin flips with that coin.\n",
    "\n",
    "### $$P(\\text{heads}) = \\lim_{\\text{# of coin flips} \\to \\infty} \\frac{\\text{# of heads}}{\\text{# of flips}}$$\n",
    "\n",
    "Alternatively, we can write this more generally as the number of times any event $A$ occurs given an infinite number of observations/experiments (random samples from the event space).\n",
    "\n",
    "### $$P(A) = \\lim_{\\text{# of experiments} \\to \\infty} \\frac{\\text{# of occurances of A}}{\\text{# of experiments}} $$\n",
    "\n",
    "### Bayesianism\n",
    "\n",
    "Bayesians believe that data informs us about the distribution of a statistic or event, and as we receive more data our view of the distribution can be updated, further confirming or denying our previous beliefs (but never in total certainty).\n",
    "\n",
    "For the coin flip example above, we would write out the probability of heads as our belief in the probability of getting heads given the evidence we have from observing coin flips.\n",
    "\n",
    "### $$ P(\\text{heads}) = \\frac{P(\\text{# of heads observed} \\;|\\; \\text{heads})}{P(\\text{# of heads observed})} P(\\text{heads}) $$\n",
    "\n",
    "Here we are representing the probability of flipping with:\n",
    "\n",
    "Our **prior** belief, before observing flips, of the probability of flipping heads: $P(\\text{heads})$\n",
    "\n",
    "The **likelihood** of the data we observe given the chance to flip heads: $P(\\text{# of heads observed} \\;|\\; \\text{heads})$\n",
    "\n",
    "The **total probability** of observing that many heads in coin flips regardless of weighting (or rather, across all coin weightings): $P(\\text{# of heads observed})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parts'></a>\n",
    "## Bayes rule in parts\n",
    "---\n",
    "\n",
    "Using the diachronic interpretation of Bayes Rule, we can describe each part with its label like in our coin flip example above.\n",
    "\n",
    "### $$P\\left(model\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;model\\right)}{P(data)}\\; P\\left(model\\right)$$\n",
    "\n",
    "**The prior**\n",
    "\n",
    "### $$ \\text{prior} = P\\left(model\\right) $$\n",
    "\n",
    "The prior is our belief in the model given no additional information. This \"model\" could be as simple as a statistic like the mean we are measuring, or a complex regression. \n",
    "\n",
    "**The likelihood**\n",
    "\n",
    "### $$ \\text{likelihood} = P\\left(data\\;|\\;model\\right) $$\n",
    "\n",
    "The likelihood is the probability of the data we observed occuring given the model. So, for example, assuming that a coin is biased towards heads with a mean rate of heads of 0.9, what is the likelihood we observed 10 tails and 2 heads in 12 coin flips.\n",
    "\n",
    "The likelihood is in fact what frequentist statistical methods are measuring. \n",
    "\n",
    "**The marginal probability or total probability of the data**\n",
    "\n",
    "### $$ \\text{marginal probability of data} = P(data) $$\n",
    "\n",
    "The marginal probability of the data is the probability that our data is observed regardless of what model we choose or believe in. You divide the likelihood by this value to ensure that we are only talking about our model within the context of the data occuring. More technically, we divide by this value to ensure that what we get out on the other side is a true probability distribution - more on this later.\n",
    "\n",
    "**The posterior**\n",
    "\n",
    "### $$ \\text{posterior} = P\\left(model\\;|\\;data\\right) $$\n",
    "\n",
    "The posterior is our _updated_ belief in the model given the new data we have observed. Bayesian statistics is all about updating a prior belief we have about the world with the data we observe, and so we are transforming our _prior_ belief about the world into this new _posterior_ belief about the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monty-hall'></a>\n",
    "\n",
    "## The Monty Hall problem\n",
    "---\n",
    "\n",
    "The Monty Hall problem is a famous probability problem with an unintuitive solution. Framing it in a Bayesian context makes it clear!\n",
    "\n",
    "[Open up the Monty Hall notebook and tackle the problem.](./monty-hall.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional'></a>\n",
    "## Additional Bayesian statistics problems\n",
    "---\n",
    "\n",
    "As independent practice, you can tackle some more Bayesian statistics problems:\n",
    "- Pregnancy screening problem\n",
    "- Cookie Jar problem\n",
    "- The German Tank problem\n",
    "- Dungeons & Dragons dice problems\n",
    "- M&M's problem\n",
    "\n",
    "[The questions can be found in this notebook.](bayes-problems.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
