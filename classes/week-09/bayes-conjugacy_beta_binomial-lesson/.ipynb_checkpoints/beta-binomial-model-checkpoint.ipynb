{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Conjugacy and the Beta-Binomial Model\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the concept of \"conjugacy\" and \"conjugate priors\" in Bayesian statistics\n",
    "- Set up an example of the Beta-Binomial model using a subscription probability example\n",
    "- Understand the Binomial likelihood and where it fits in Bayes formula\n",
    "- Calculate the Maximum Likelihood Estimate\n",
    "- Understand when and why the MLE point estimate is insufficient\n",
    "- Use the Beta-Binomial model to build our example in a Bayesian framework\n",
    "- Understand and visualize the Beta distribution\n",
    "- Learn about the Beta function, Gamma function, and where they fit in the Beta distribution calculation\n",
    "- Mathematically derive the conjugacy relationship between the prior and posterior of the Beta-Binomial model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [Review: the Binomial distribution probability mass function](#pmf)\n",
    "- [Modeling the parameter $p$ given counts of successes and failures](#p)\n",
    "- [The Binomial likelihood](#likelihood)\n",
    "- [The \"Maximum Likelihood Estimate\" for $p$](#mle)\n",
    "    - [The likelihood function](#likelihood-func)\n",
    "    - [When the MLE does not make sense](#nonsense)\n",
    "- [Bayesian modeling of the parameter $p$ and the Beta distribution](#beta)\n",
    "- [The Beta PDF and the Beta function](#beta-pdf)\n",
    "- [The Gamma function](#gamma)\n",
    "- [Defining the Beta function in terms of the Gamma function](#beta-gamma)\n",
    "- [Putting it all together: the Beta as a \"conjugate prior\" to the Binomial likelihood](#beta-conjugate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "**Conjugacy** and \"conjugate priors\" are important concepts in Bayesian statistics. The essential idea is that the *posterior* distribution is guaranteed to have the same form as the *prior* distribution when the prior distribution is a conjugate prior to the likelihood function.\n",
    "\n",
    "There are many conjugate priors and posteriors. They are extremely useful because they make the prior-posterior update algebraically solveable. When there is no conjugate prior, sampling techinques such as Markov Chain Monte Carlo are often necessary.\n",
    "\n",
    "This lecture covers the most classic conjugate prior scenario: the Beta-Binomial model. Binomial models are appropriate for binary events. The prior distribution on the probability of a binary event is a Beta distribution. As it turns out, the Beta distribution is conjugate to the Binomial likelihood and we are guaranteed to get out a posterior distribution that is also a Beta distribution.\n",
    "\n",
    "If that all sounds like nonsense right now: don't worry. We will be walking through this in great detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pmf'></a>\n",
    "## Review: the Binomial distribution probability mass function\n",
    "---\n",
    "\n",
    "Recall that the number of \"success\" trials in $n$ trials is modeled with the Binomial distribution. The binomial distribution has the probability mass function:\n",
    "\n",
    "### $$ P(k \\;|\\; n, p) = \\binom{n}{k} p^k (1 - p)^{(n-k)} $$\n",
    "\n",
    "Where $k$ is the number of successes,\n",
    "\n",
    "$n$ is the number of total trials,\n",
    "\n",
    "and $p$ is the probability of success on each trial.\n",
    "\n",
    "**We can plot the probability mass function for a given $n$ and $p$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we change the probability of success $p$ (or if we wanted to the total trials $n$) we can see that the probability mass function changes - values of $k$ have different probabilities or likelihoods of occuring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='p'></a>\n",
    "## Modeling the parameter $p$ given counts of successes and failures\n",
    "---\n",
    "\n",
    "Let's reframe this. \n",
    "\n",
    "Say instead that we were measuring visitors to our site and also whether they chose to subscribe to our newsletter or not. So we redefine $n$, $k$, and $p$ accordingly:\n",
    "\n",
    "### $$ \\begin{aligned} n &= \\text{number of visitors to our website} \\\\\n",
    "k &= \\text{number of visitors who subscribed} \\\\\n",
    "p &= \\text{probability of a visitor subscribing (unknown)} \\end{aligned}$$\n",
    "\n",
    "Remember, now we are _measuring_ $k$ subscribers out of the $n$ visitors. The measurement of subscribers can be considered our data.\n",
    "\n",
    "At this point, we want to make an inference about the parameter $p$, our probability of a visitor subscribing. We can talk about this in terms of Bayes' Theorem:\n",
    "\n",
    "### $$ P(p \\;|\\; data) = \\frac{ P(data \\;|\\; p) }{ P(data) } P(p) $$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) = \\frac{ P(n,k \\;|\\; p) }{ P(n, k) } P(p) $$\n",
    "\n",
    "Where we have:\n",
    "\n",
    "### $$ \\begin{aligned} \n",
    "P(p \\;|\\; n,k) &= \\text{posterior} \\\\\n",
    "P(n,k \\;|\\; p) &= \\text{likelihood} \\\\\n",
    "P(n,k) &= \\text{marginal probability of the data} \\\\\n",
    "P(p) &= \\text{prior} \n",
    "\\end{aligned} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood'></a>\n",
    "## The Binomial likelihood\n",
    "---\n",
    "\n",
    "Let's start with the likelihood. The likelihood represents the probability of observing $k$ successes out of $n$ trials _given a probability of success $p$._\n",
    "\n",
    "This $p$ can be fixed, say at $p = 0.3$, in which case we would evaluate the likelihood at exactly that point. We could also represent $p$ as a distribution over the range of possible $p$ values. Like in the locomotive example, evaluating the likelihood at all of our different \"hypotheses\" about what $p$ could be. \n",
    "\n",
    "Let's start with a fixed value, $p = 0.3$. How do we evaluate the likelihood? As it turns out the likelihood function is the same as the probability mass function we wrote above, because this function is literally used to evaluate \"what is the probability of $k$ successes given $n$ trials and $p$ probability of success\". This is what we have formulated as the likelihood in the numerator.\n",
    "\n",
    "**So we can use the binomial object initialized with $p = 0.3$ and $n = 25$ to find the likelihood value for a given $k$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mle'></a>\n",
    "## The \"Maximum Likelihood Estimate\" for $p$\n",
    "---\n",
    "\n",
    "If we were to _just_ focus on the likelihood part of Bayes Theorem, we could ask \"what is the value of the parameter $p$ that maximizes the value of the likelihood function?\" This is precisely what we do in Frequentist statistics to find our point estimate of a parameter. \n",
    "\n",
    "Remember that Frequentists have no interest in the prior or posterior beliefs about the probability of the parameter's value. Frequentists state that there is no probability associated with a parameter (such as our probability of subscription). There is one, _true_ probability of subscription if we were to measure the entire population. \n",
    "\n",
    "Because we only take a sample of people, we may by chance measure a probability of subscription that deviates from that true probability to some degree. Remember: in Frequentist statistics, it is the data that has a probability rather than the parameter!\n",
    "\n",
    "**For the Binomial distribution, we can easily calculate the value for subscription rate $p$ that makes our observed data the most likely: it is going to be the fraction of successes that we measured in our data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood-func'></a>\n",
    "### The likelihood function\n",
    "\n",
    "But we can also derive the MLE more formally. Our scenario is simple, but for distributions and models that are not so simple this becomes necessary.\n",
    "\n",
    "**First, define the likelihood function $L$ (which is te same as the PMF):**\n",
    "\n",
    "### $$ L(n, k \\;|\\; p) = \\binom{n}{k} p^k (1 - p)^{(n-k)} $$\n",
    "\n",
    "**Take the logarithm of this to get the log likelihood.**\n",
    "\n",
    "### $$ LL(n, k \\;|\\; p) = ln\\binom{n}{k} + k \\cdot ln(p) + (n - k) \\cdot ln(1 - p) $$\n",
    "\n",
    "\n",
    "The log likelihood has nice properties. It allows the computer do do computations with very small probabilities multiplied together. It also gets rid of our exponents, which makes the derivative easier.\n",
    "\n",
    "**Now take the derivative of the log likelihood with respect to $p$ and set this to 0.** This will find the value of $p$ that maximizes the log likelihood (the likelihood function is convex).\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "\\frac{\\partial}{\\partial p} LL(n, k \\;|\\; p) &= 0 \\\\\n",
    "\\frac{k}{p} - \\frac{(n-k)}{(1-p)} &= 0 \\\\\n",
    "\\frac{(n-k)}{(1-p)} &= \\frac{k}{p} \\\\\n",
    "pn - pk &= k - pk \\\\\n",
    "pn - pk + pk &= k \\\\\n",
    "p &= \\frac{k}{n} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**As you can see, this distills down to what we calculated before: the fraction of users that subscribed is the maximum likelihood estimate for the subscription rate.**\n",
    "\n",
    "<a id='nonsense'></a>\n",
    "### When the MLE does not make sense\n",
    "\n",
    "Now say instead we had $n = 5$ visitors to the site and to our surprise all of them subscribed, $k = 5$. Using the MLE for $p$ we would conclude that $p = 1.0$: a person has a %100 probability of subscribing when reaching our site.\n",
    "\n",
    "Of course this is a very flawed conclusion. We have only measured 5 people! \n",
    "\n",
    "> **Note:** If we took the Frequentist route, we would ask \"what is the probability that we measured this parameter $p = 1.0$ by chance when in fact the true rate is (some predetermined null hypothesis value) $H0_p = 0.3$?\" This would be our p-value, a.k.a. alpha or Type I error, and with such insufficient data we would almost certainly fail to reject the null hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta'></a>\n",
    "\n",
    "## Bayesian modeling of the parameter $p$ and the Beta distribution\n",
    "---\n",
    "\n",
    "What if we took a Bayesian rather than Frequentist approach?\n",
    "\n",
    "Now, instead of thinking about the *data* as having a probability we think of the *parameter* $p$ as having a probability. In other words, different values of $p$ have different _likelihoods_. We will represent our beliefs about likely values of $p$ with our prior distribution.\n",
    "\n",
    "**The distribution that represents _a distribution of probabilities_ is the Beta distribution. The beta distribution is parameterized by two values, $\\alpha$ and $\\beta$.**\n",
    "\n",
    "###  $$ Beta(\\alpha, \\beta) =\n",
    "\\begin{cases}\n",
    "\\alpha &= \\text{number of successes + 1} \\\\\n",
    "\\beta &= \\text{number of failures + 1}\n",
    "\\end{cases} $$\n",
    "\n",
    "**We can plot the beta distribution for the scenario where we measured $k = 5$ out of $n = 5$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from this distribution that our probability with the highest likelihood is 1.0. But, other probabilities are also likely!**  Due to our low sample size $n$, many values other than $p = 1.0$ have reasonable likelihood.\n",
    "\n",
    "**What if we measured 20 subscriptions out of 20 visitors?** Plot this scenario below to see how the beta distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-pdf'></a>\n",
    "## The Beta PDF and the Beta function\n",
    "---\n",
    "\n",
    "This is all well and good, but how is the Beta distribution defined? Formally, we define the probability density function of the beta distribution as:\n",
    "\n",
    "### $$ PDF_{Beta}(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int_0^1 u^{\\alpha-1} (1-u)^{\\beta-1}\\, du} $$\n",
    "\n",
    "Where $x$ falls in the range [0, 1], and $u$ represents the values in that range to integrate over.\n",
    "\n",
    "In the denominator, we are integrating over the possible probabilities. The denominator of the PDF is actually called the \"Beta function\", not to be confused with the Beta _distribution_. \n",
    "\n",
    "If this looks familiar to the equation for the binomial likelihood above, it's because it is. In the numerator we essentially have the binomial likelihood equation but with the \"shape\" parameters $\\alpha$ and $\\beta$ in place of our $k$ and $n$. In the denominator, we are integrating the binomial likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gamma'></a>\n",
    "## The Gamma function\n",
    "---\n",
    "\n",
    "There is another way to write the Beta distribution, using something called the \"Gamma function\". \n",
    "\n",
    "The gamma function is defined as:\n",
    "\n",
    "###  $$ \\Gamma(z) =\n",
    "\\begin{cases}\n",
    "(z - 1)! &= \\text{when z is a positive integer} \\\\\n",
    "\\int_0^{\\infty} x^{z-1} e^{-x} dx &= \\text{when z is a complex real number}\n",
    "\\end{cases} $$\n",
    "\n",
    "**We can plot out a gamma distribution below for say, $z = 10$, using scipy's gamma object:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-gamma'></a>\n",
    "## Definining the Beta function using the Gamma function\n",
    "---\n",
    "\n",
    "The Gamma function is a generalization of the factorial function. The Beta _function_ can also be written in terms of the Gamma function:\n",
    "\n",
    "### $$ Beta(\\alpha, \\beta) = \\frac{ \\Gamma (\\alpha) \\Gamma (\\beta) }{\\Gamma (\\alpha + \\beta) } = \\int_0^1 u^{\\alpha-1} (1-u)^{\\beta-1}\\, du $$\n",
    "\n",
    "At this point, we can rewrite the Beta _distribution_, or probability density function, like so:\n",
    "\n",
    "### $$ PDF_{Beta}(x) = \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) }x^{\\alpha-1}(1-x)^{\\beta-1} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-conjugate'></a>\n",
    "## Putting it all together: the Beta as a \"conjugate prior\" to the Binomial likelihood\n",
    "---\n",
    "\n",
    "So remember - our beta distribution is what we are going to be using as our _prior_ over the probability of subscription $p$. In other words, we have some distribution of beliefs about which subscription rates are most likely as represented by a beta distribution.\n",
    "\n",
    "**Recall now the setup of this problem in terms of Bayes formula:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) = \\frac{ P(n,k \\;|\\; p) }{ P(k, n) } P(p) $$\n",
    "\n",
    "**Let's ignore the normalizing constant, the marginal probability of the data $k,n$, for now. We can say then the unnormalized posterior is:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) \\propto P(n,k \\;|\\; p) \\cdot P(p) $$\n",
    "\n",
    "**And we can put our Binomial likelihood and the Beta posterior in where we had the placeholders:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) \\propto \\binom{n}{k} p^k (1 - p)^{(n-k)} \\cdot \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) }p^{\\alpha-1}(1-p)^{\\beta-1} $$\n",
    "\n",
    "**Let's now define a constant $c$ as:**\n",
    "\n",
    "### $$ c = \\binom{n}{k} \\cdot \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) } $$\n",
    "\n",
    "**Now our formula for the unnormalized posterior is:**\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "P(p \\;|\\; n,k) &\\propto c \\cdot p^k (1 - p)^{(n-k)} \\cdot p^{\\alpha-1}(1-p)^{\\beta-1} \\\\\n",
    "P(p \\;|\\; n,k) &\\propto c \\cdot p^{(k + \\alpha - 1)} (1-p)^{(n - k + \\beta - 1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**And if we define a new alpha and beta:**\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "\\alpha_{posterior} &= k + \\alpha_{prior} \\\\\n",
    "\\beta_{posterior} &= n - k + \\beta_{prior}\n",
    "\\end{aligned} $$\n",
    "\n",
    "**We can see that the posterior distribution can in fact be parameterized as a Beta distribution.** The constant term $c$ will be handled when we put the marginal likelihood back in and normalize the posterior distribution to be a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
