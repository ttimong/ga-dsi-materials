{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> **Jupyter slideshow:** This notebook can be displayed as slides. To view it as a slideshow in your browser type in the console:\n",
    "\n",
    "\n",
    "> `> jupyter nbconvert [this_notebook.ipynb] --to slides --post serve`\n",
    "\n",
    "\n",
    "> To toggle off the slideshow cell formatting, click the `CellToolbar` button, then `View --> Cell Toolbar --> None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Support Vector Machines (SVM)\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "- Understand how the SVM builds its decision threshold.\n",
    "- Understand the concept of the maximum margin hyperplane.\n",
    "- Visualize the linearly separable case in classification.\n",
    "- Understand the math behind finding the maximum margin hyperplane.\n",
    "- Understand the hinge loss for SVM.\n",
    "- Understand how the regularization constant C allows SVMs to fit non-linearly separable problems.\n",
    "- See how the kernel trick transforms problems from non-linearly separable to linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to SVMs](#intro)\n",
    "- [How does the SVM classifiy?](#how-classify)\n",
    "- [Intuition behind the decision boundary](#intuition)\n",
    "- [Why maximize the margin?](#why-max-margin)\n",
    "- [SVM origins: the perceptron algorithm](#perceptron)\n",
    "- [The maximum margin hyperplane](#mmh)\n",
    "    - [Finding the maximum margin](#finding-mmh)\n",
    "- [The hinge loss](#hinge)\n",
    "    - [\"Slack\"](#slack)\n",
    "    - [The regularizing hyperparameter C](#hyper-c)\n",
    "- [The kernel trick](#kernel)\n",
    "- [Pros and cons](#pros-and-cons)\n",
    "- [When to use SVM vs. Logistic Regression](#when-to-use-svm)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to SVMs\n",
    "\n",
    "---\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is a different approach to classification.\n",
    "\n",
    "SVM still fits a decision boundary like a logistic regression regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "This lesson will overview the details of the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-classify'></a>\n",
    "\n",
    "## How does the SVM classify?\n",
    "\n",
    "---\n",
    "\n",
    "It's important to start with the intuition for SVM with the special _**linearly separable**_ classification case.\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./assets/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intuition'></a>\n",
    "\n",
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why-max-margin'></a>\n",
    "\n",
    "## Why maximize the margin?\n",
    "\n",
    "---\n",
    "\n",
    "**SVM solves for a decision boundary that should theoretically minimize the generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the observations with the most \"ambiguous\" labels. They are the observations that are approaching equal probability to be one class or the other (given the predictors).\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. It's decision boundary is safe in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='perceptron'></a>\n",
    "\n",
    "## SVM origins: the perceptron algorithm\n",
    "\n",
    "---\n",
    "\n",
    "The perceptron algorithm is a linear function of weights $w$ and predictors $X$ that assigns points to binary classes. If the function returns a value greater than 0 then the observation is classified as 1, otherwise it is classified as zero.\n",
    "\n",
    "$f(X)$ below is the perceptron function to determine the classes. $b$ here is known as the \"bias\", which is analagous to the intercept term.\n",
    "\n",
    "### $$ f(X) = \\begin{cases}1 & \\text{if }w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases} $$\n",
    "\n",
    "If the points are linearly seperable, the solving the perceptron is guaranteed to converge on a solution, but that solution is not necessarily optimal for future observations. This led to the invention of the SVM, which finds the optimal discriminator: the maximum margin hyperplane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mmh'></a>\n",
    "\n",
    "## The maximum margin hyperplane\n",
    "\n",
    "---\n",
    "\n",
    "We choose a normalizing constant such that the distance from the plane to the closest points of either classes will be 1.\n",
    "\n",
    "### $$ w^T x_{+} + b = 1 \\\\ w^T x_{-} + b = -1 $$\n",
    "\n",
    "For the distance to the closest positive and negative class points, respectively (known as \"support vectors\").\n",
    "\n",
    "If the normalizer for the weights is $||w||$, the size of the margin is then:\n",
    "\n",
    "### $$ \\text{margin} = \\frac{2}{||w||} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='finding-mmh'></a>\n",
    "### Finding the maximum margin\n",
    "\n",
    "We want to find a distance between these points that is the widest possible. Therefore, we are looking to maximize the value $\\frac{2}{||w||}$.\n",
    "\n",
    "So, maximize the weights with constraints on what the function can output. When the target is 1, the function needs to be 1 or greater. When the target is 0 (or -1), the function needs to output -1 or lower.\n",
    "\n",
    "### $$ \\underset{w}{\\text{max}} \\frac{2}{||w||} \\quad \\text{subject to} \\begin{cases}\\text{if } y_i = 1 & w^T x_i + b \\ge 1 \\\\ \\text{if } y_i = -1 & w^T x_i + b \\le -1 \\end{cases} \\text{for } i \\text{ in } N$$\n",
    "\n",
    "Note here that $y$ is specified as either 1 or -1, as opposed to the 0, 1 that we are used to. This is convenient for converting this to be a minimization. To make this a minimization, we can change the equation to be:\n",
    "\n",
    "### $$ \\underset{w}{\\text{min }} ||w||^2 \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 $$\n",
    "\n",
    "Which works out because when $y = -1$ the values become positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hinge'></a>\n",
    "\n",
    "## The hinge loss and non-linearly separable cases\n",
    "\n",
    "---\n",
    "\n",
    "In cases where there is no line or plane that can separate all of the points perfectly, we need to introduce the capacity for model error. Using the constraint above that $y(w^T X + b) \\ge 1$, we can introduce the **hinge loss function**:\n",
    "\n",
    "### $$ \\text{hinge loss} = \\sum_{i=1}^n \\max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "Where now, if the point is on the correct side (correctly classified), the value will be 0. If the point is not $\\ge 1$, this function will be greater than zero.\n",
    "\n",
    "Using $f(x_i) = (w^T x_i + b)$, \n",
    "\n",
    "### $$\\text{hinge loss} = \\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "If $f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hinge loss](./assets/hinge_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='slack'></a>\n",
    "\n",
    "### Hinge loss and \"slack\"\n",
    "\n",
    "When we have a scenario where it is not possible to perfectly separate, we use the hinge loss with a regularization constant $C$:\n",
    "\n",
    "### $$ \\min ||w||^2 + C\\sum_{i=1}^N \\epsilon_i \\quad \\text{subject to} \\quad y(w^T x_i + b) \\ge 1 - \\epsilon_i $$\n",
    "\n",
    "Where the $\\epsilon$ are the errors of the classifier, and the $C$ is a regularization term that determines how much the classification errors matter (relative to the maximization of the margin).\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "### $$  \\underset{w}{\\text{min }} ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "A small $C$ creates a wider margin because errors will matter less. A large $C$ creates a tighter margin because errors matter more. An infinite $C$ parameter is a \"hard\" margin, which always minimizes error over the size of the boundary.\n",
    "\n",
    "We are trying to minimize the norm of the weights $||w||$ and thus maximize the margin, but now we are also trying to minimize our errors at the same time. We have a balance in our minimization between how wide the margin should be and how much error we tolerate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hyper-c'></a>\n",
    "### The regularizing hyper-parameter $C$\n",
    "\n",
    "By setting the hyper-parameter $C$ we can control the extent to which our SVM is tolerant to misclassification errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\", similar to the lambda terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside of the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observations errors are allowed. These misclassified points are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exporing the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/slack_variables.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function(np.array([xi, yj]).reshape(1, -1))\n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('X1', fontsize=14)\n",
    "ax.set_ylabel('X2', fontsize=14)\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=121, cluster_std=0.60)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.flag, edgecolors='black', linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/soft_margin.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_linear_svc(C):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X, y)\n",
    "    plt.title('Linear, C={}'.format(C))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('X1', fontsize=14)\n",
    "    ax.set_ylabel('X2', fontsize=14)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.flag, edgecolors='black', linewidth=1)\n",
    "    plot_svc_decision_function(clf);\n",
    "    \n",
    "def plot_interactive_linear_svc(X, y):\n",
    "    def interactive_linear_svc(C1, C2):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plot_linear_svc(C=C1)\n",
    "        plt.subplot(122)\n",
    "        plot_linear_svc(C=C2)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    interact(interactive_linear_svc, C1=(.1, 1),  C2=(1, 10))\n",
    "    \n",
    "plot_interactive_linear_svc(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kernel'></a>\n",
    "## The \"kernel trick\" for non-linearly separable problems\n",
    "\n",
    "---\n",
    "\n",
    "The \"kernel trick\" allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "The idea behind the kernel trick is that you can arbitrarily transform your observations that _have no linear separability_ by putting them into a different \"dimensional space\" where the DO have linear separability, fit an SVM in that higher dimensional space, and then invert the transformation of the data and the model itself back into the original space.\n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them into this higher dimensional space. \n",
    "\n",
    "[Check out these lecture slides for more detail.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you a general intuition for what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./assets/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_poly_svc(gamma, C):\n",
    "    clf = SVC(kernel='poly', degree=gamma, C=C)\n",
    "    clf.fit(X, y)\n",
    "    plt.title('poly={}, C={}'.format(gamma, C))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('X1', fontsize=14)\n",
    "    ax.set_ylabel('X2', fontsize=14)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.flag, edgecolors='black', linewidth=1)\n",
    "    plot_svc_decision_function(clf);\n",
    "\n",
    "def plot_interactive_poly_svc(X, y):\n",
    "    def interactive_poly_svc(C, poly):\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plot_poly_svc(poly, C)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    interact(interactive_poly_svc, C=(.1, 10), poly=(1,5))\n",
    "    \n",
    "plot_interactive_poly_svc(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./assets/nonlinear-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_rbf_svc(gamma, C):\n",
    "    clf = SVC(kernel='rbf', gamma=gamma, C=C)\n",
    "    clf.fit(X, y)\n",
    "    plt.title('RBF, gamma={}, C={}'.format(gamma, C))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('X1', fontsize=14)\n",
    "    ax.set_ylabel('X2', fontsize=14)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.flag, edgecolors='black', linewidth=1)\n",
    "    plot_svc_decision_function(clf);\n",
    "    \n",
    "def plot_interactive_rbf_svc(X, y):\n",
    "    def interactive_rbf_svc(C, gamma1, gamma2, gamma3, gamma4):\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.subplot(221)\n",
    "        plot_rbf_svc(gamma1, C)\n",
    "        plt.subplot(222)\n",
    "        plot_rbf_svc(gamma2, C)\n",
    "        plt.subplot(223)\n",
    "        plot_rbf_svc(gamma3, C)\n",
    "        plt.subplot(224)\n",
    "        plot_rbf_svc(gamma4, C)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    interact(interactive_rbf_svc, C=(.1, 10), gamma1=(.1, 1), \n",
    "                    gamma2=(1, 10), gamma3=(10, 100), gamma4=(100, 200))\n",
    "    \n",
    "plot_interactive_rbf_svc(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pros-and-cons\"></a>\n",
    "## Pros and cons\n",
    "**Pros**\n",
    "- Exceptional perfomance (historically widely used)\n",
    "- Robust to outliers\n",
    "- Effective in high dimensional data\n",
    "- Can work with non-linearities\n",
    "- Fast to compute even on non-linear (kernel trick)\n",
    "- Low risk of overfitting\n",
    "\n",
    "**Cons**\n",
    "- Blackbox\n",
    "- Can be slow on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"when-to-use-svm\"></a>\n",
    "## When to use SVM vs. Logistic Regression\n",
    "Advice from Andrew Ng:\n",
    "\n",
    "**If there are more feature than training samples:**\n",
    "    Use logistic regression or SVM without a kernel (\"linear kernel\")\n",
    "    \n",
    "**If there are about 10 times as many samples as features:**\n",
    "    Use SVM with a Gaussian kernel\n",
    "    \n",
    "**If there are many more training samples than features:**\n",
    "    Spend time feature engineering, then use logistic regression or SVM\n",
    "    without a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "- [This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n",
    "- SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "- Iris example on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py)\n",
    "- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)\n",
    "- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to SVM. My fav!\n",
    "- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane\n",
    "- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview plus a [20min video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core \"need-to-knows\"\n",
    "- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM\n",
    "- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the highnotes of SVM\n",
    "- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on SVM from CS 229\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=eHsErlPJWUU) (1hr+) from one of my fav lecturers (Dr Yasser) on SVM. He does a followup on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50min) (from MIT Opencoursewar)\n",
    "- An infamous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 7000+ times!) on why SVM is a great text classifier\n",
    "- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
