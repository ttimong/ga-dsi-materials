{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Web Scraping and Spiders with `scrapy`\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Sam Stack(DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the structure and content of HTML\n",
    "- Learn about elements, attributes, and element hierarchy in HTML\n",
    "- Learn about XPath and using multiple and singular selections\n",
    "- Practice using Scrapy to get data from craigslist\n",
    "- Practice using Beautiful Soup to parse data from craigslist\n",
    "- Walkthrough the construction of a spider built using scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "- [HTML](#html)\n",
    "    - [Elements](#elements)\n",
    "    - [Attributes](#attributes)\n",
    "    - [Element hierarchy](#element-hierarchy)\n",
    "    - [More resources on HTML structure](#html-resources)\n",
    "- [What is XPath?](#xpath)\n",
    "    - [Multiple selections](#multiple-selections)\n",
    "    - [Singular selections](#singlular-selections)\n",
    "- [A simple `scrapy` example](#scrapy)\n",
    "- [A practical example with Requests and Beautiful Soup](#practical)\n",
    "    - [Step 1: fetch the content by URL](#step1)\n",
    "    - [Step 2: Parse HTML document with Beautiful Soup](#step2)\n",
    "    - [Practice: can you select the price of our junker?](#practice)\n",
    "- [Scrapy and spiders](#scrapy)\n",
    "    - [Create a Scrapy project](#scrapy-project)\n",
    "    - [Define an \"item\"](#define-item)\n",
    "    - [A spider that crawls](#spider-crawl)\n",
    "    - [XPath and parsing with our spider](#xpath-spider)\n",
    "    - [Save and examine our scraped data](#save-examine)\n",
    "- [Addendum: leveraging XPath to get more results](#addendum)\n",
    "    - [Following links](#follow-links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#introduction)\n",
    "- [HTML](#html)\n",
    "    - [Elements](#elements)\n",
    "    - [Attributes](#attributes)\n",
    "- [What is XPath?](#xpath)\n",
    "    - [Absolute References](#xpath_absolute)\n",
    "    - [Relative References](#xpath_relative)\n",
    "    - [\"Wheres Waldo?\" Exercise](#waldo_exercise)\n",
    "- [1 vs N Selectors](#1_v_n)\n",
    "- [Demo Code](#demo)\n",
    "    - [Scrape Data Tau](#scrape_tau)\n",
    "- [Independent Practice](#ind_practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "![What is Html](http://designshack.designshack.netdna-cdn.com/wp-content/uploads/htmlbasics-0.jpg)\n",
    "\n",
    "One of the largest sources of data in the world is all around us.  We consume the web in some form every day.  One of the most powerful python toolsets we will learn allows us to extract and normalize data from unstructured sources like webpages.  \n",
    "\n",
    "**If you can see it, it can be scraped, mined, and put into a dataframe.**\n",
    "\n",
    "Before we begin the actual process of webscraping with python, it is important to cover the basic constructs that describe HTML as unstructured data. \n",
    "\n",
    "Then we will cover a a powerful selection technique called XPath, and look at a basic workflow using a framework called [Scrapy](http://www.scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html'></a>\n",
    "\n",
    "## Hypertext markup language (HTML)\n",
    "\n",
    "---\n",
    "\n",
    "In the HTML DOM (Document Object Model), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elements'></a>\n",
    "### Elements\n",
    "Elements begin and end with open and close \"tags\", which are defined by namespaced, encapsulated strings. These namespaces that begin and end the elements must be the same.\n",
    "\n",
    "```\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "As you may have several different titles or paragraphs on a single page, you can assign ID values to namespace to make more unique reference points.  IDs are also very useful for labelling nested elements.\n",
    "```\n",
    "<title id ='title_1'>I am a the first title.</title>\n",
    "<p id ='para_1'>I am the first paragraph.</p>\n",
    "<title id ='title_2'>I am a the second title.</title>\n",
    "<p id ='para_2'>I am the second paragraph.</p>\n",
    "```\n",
    "\n",
    "\n",
    "**Elements can have parents and children:**\n",
    "It is important to remember that an element can be both a parent and a child and whether to refer to the element as a parent or a child depends on the specific element you are referencing.\n",
    "\n",
    "\n",
    "```\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the child of 'parent'\n",
    "        <div id = 'child_2'>I am the child of 'child_1'\n",
    "            <div id = 'child_3'>I am the child of 'child_2'\n",
    "                <div id = 'child_4'>I am the child of 'child_4'</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "**or**\n",
    "```\n",
    "<body id = 'parent'>\n",
    "    <div id = 'child_1'>I am the parent of 'child_2'\n",
    "        <div id = 'child_2'>I am the parent of 'child_3'\n",
    "            <div id = 'child_3'> I am the parent of 'child_4'\n",
    "                <div id = 'child_4'>I am not a parent </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>\n",
    "### Attributes\n",
    "\n",
    "HTML elements can have attributes.  They describe properties, and characteristics of elements.  Some affect how the element behaves or looks in terms of the rendered output by the browser.\n",
    "\n",
    "The most common element is an \"anchor\" element.  Anchor elements often have an \"href\" element, which tells the browser where to go after it is clicked.  Anchor elements are typically are formatted in bold, and sometimes are underlined as a visual cue to differentiate itself.\n",
    "\n",
    "**Markup that describes nn element with attributes, litterally looks like this**\n",
    "\n",
    "```\n",
    "<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">An Awesome Website</a>\n",
    "```\n",
    "\n",
    "**However, this element, once rendered, looks like this**\n",
    "\n",
    "[An Awesome Website](https://www.youtube.com/watch?v=dQw4w9WgXcQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='element-hierarchy'></a>\n",
    "### Element hierarchy\n",
    "\n",
    "![Nodes](http://www.computerhope.com/jargon/d/dom1.jpg)\n",
    "\n",
    "**Literally Represented:**\n",
    "\n",
    "```\n",
    "<html>\n",
    "    \n",
    "    <head>\n",
    "        <title>Example</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "        <h1>Example Page</h1>\n",
    "        <p>This is an example page.</p>\n",
    "    </body>\n",
    "    \n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='html-resources'></a>\n",
    "### You are now qualified HTML experts\n",
    "\n",
    "![](assets/certified.jpg)\n",
    "\n",
    "Your HTML learning can continue...\n",
    "\n",
    "Read all about the different elements supported amongst modern browsers:\n",
    " * [HTML5 Cheatsheet](http://websitesetup.org/html5-cheat-sheet/)\n",
    " * [Mozilla HTML Element Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    " * [HTML5 Visual Cheatsheet](http://www.unitedleather.biz/PDF/HTML5-Visual-Cheat-Sheet1.pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xpath'></a>\n",
    "\n",
    "## What is XPath?\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/obama_wiki.png)\n",
    "\n",
    "Understanding how to identify elements and attributes within HTML documents gives us the capability to write simple expressions that create structured data.  We can think os XPath like a query language for querying HTML.\n",
    "\n",
    "To make this process easier to deal with, we will be using XPath helper, which is a Chrome addon.  It's not necessary, but highly recommended to help build XPath expressions.\n",
    "\n",
    "[XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
    "\n",
    "XPath expressions can select elements, element attributes, and element text.  These selections can be either to a single item, or multiple items.  Generally, if you're not specific enough, you will end up selecting multiple elements.\n",
    "\n",
    "\n",
    "<a id='multiple-selections'></a>\n",
    "### Multiple selections\n",
    "\n",
    "***Multiple selections*** are useful for capturing search results, or any repeating element.  For instance, the _titles_ of an apartment listing search results from Craigslist:\n",
    "\n",
    "\n",
    "**URL**\n",
    "\n",
    "[http://sfbay.craigslist.org/search/sfc/apa](http://sfbay.craigslist.org/search/sfc/apa)\n",
    "\n",
    "\n",
    "**Example HTML Markup**\n",
    "```\n",
    "...\n",
    "<span class=\"pl\"> \n",
    "    <time datetime=\"2016-01-12 23:27\" title=\"Tue 12 Jan 11:27:35 PM\">Jan 12</time> \n",
    "    <a href=\"/sfc/apa/5400584579.html\" data-id=\"5400584579\" class=\"hdrlnk\">Welcome home to a sweetly renovated four bedroom one and a half bath</a> \n",
    "</span>\n",
    "...\n",
    "```\n",
    "\n",
    "**XPath - Multiple Titles** _copy this into the XPath Helper Query box_\n",
    "```\n",
    "//a[@class='result-title hdrlnk']\n",
    "```\n",
    "\n",
    "**Returns (Ad Titles)**\n",
    "```\n",
    "***New Remodeled two bedroom Apartment***\n",
    "WONDERFUL ONE BR APARTMENT HOME\n",
    "Beautiful 1bed/1bath Apartment in Russian Hill NO SECURITY DEPOSIT\n",
    "Knockout SF View|Green Oasis|Private Driveway|Furnished\n",
    "3BR/3BA Spacious, Beautiful SOMA Loft: 5 month lease\n",
    "Nob Hill Large Studio - Light, Quiet, Lovely Building\n",
    "etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='singlular-selections'></a>\n",
    "\n",
    "### Singular selections\n",
    "\n",
    "***Singular selections*** are necessary when you want to grab specific, unique text within elements.  Here's an example of a details page on Craigslist:\n",
    "\n",
    "> *Note: this example may be expired if you view it sometime after Jan 12th, 2016. Please replace this with a current craigslist listing!\n",
    "\n",
    "**URL**\n",
    "\n",
    "[https://sfbay.craigslist.org/sfc/apa/6161864063.html](https://sfbay.craigslist.org/sfc/apa/6161864063.html)\n",
    "\n",
    "**HTML Markup**\n",
    "\n",
    "```\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "**XPath - Single Item**\n",
    "\n",
    "```\n",
    "//p[@class='postinginfo'][2]/time\n",
    "```\n",
    "**Returns (Time of posting or age of Post)**\n",
    "```\n",
    "2016-01-12 11:23pm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "\n",
    "## A simple  example using `scrapy` and `XPath`.\n",
    "\n",
    "---\n",
    "\n",
    "Below is an example of how to get information out of some fake HTML using the XPath capabilities of the `scrapy` package. You will likely need to install the scrapy package using `conda install scrapy`.   \n",
    "**Note:** `Conda install` will install the necessary dependent packages needed for Scrapy, `pip install` will **not**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Selector` class from the `Scrapy` library to help us construct our query.\n",
    "\n",
    "`Selector` classes take the HTML target as an argument and can then utilize several flavors of query types to extract information.  In our situation we will specify `XPath` as our query will utilize XPath flavoured language. \n",
    "\n",
    "Just like with writing python scripts, there are several was you can access the exact same information in HTML.  Lets try a few out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "# HTML structure string\n",
    "HTML = \"\"\"\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: use the exact class name to get its associated text\n",
    "best = Selector(text=HTML).xpath(\"//span[@class='bestof-text']/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: use the 'contains()' function extract any text that includes the text 'best of'\n",
    "best = Selector(text=HTML).xpath(\"//span[contains(text(), 'best of')]/text()\").extract()\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 3: First grabs the entire html post where 'class='bestof-link'\n",
    "best =  Selector(text=HTML).xpath(\"/html/body/div/p/a[@class='bestof-link']\")\n",
    "        # parse the first grabbed chunk for the the text of the specific element with class='bestof-text'\n",
    "nested_best =  best.xpath(\"./span[@class='bestof-text']/text()\").extract()\n",
    "nested_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Option 3 will probably be the most common for you because there is a good chance that you will want to grab information from several children elements that exist within one parent element._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where's Waldo - \"XPath Edition\"\n",
    "\n",
    "In this example, we will find Waldo together.  Find Waldo as:\n",
    "\n",
    "- Element\n",
    "- Attribute\n",
    "- Text element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        \n",
    "        <ul id=\"waldo\">\n",
    "            <li class=\"waldo\">\n",
    "                <span> yo Im not here</span>\n",
    "            </li>\n",
    "            <li class=\"waldo\">Height:  ???</li>\n",
    "            <li class=\"waldo\">Weight:  ???</li>\n",
    "            <li class=\"waldo\">Last Location:  ???</li>\n",
    "            <li class=\"nerds\">\n",
    "                <div class=\"alpha\">Bill gates</div>\n",
    "                <div class=\"alpha\">Zuckerberg</div>\n",
    "                <div class=\"beta\">Theil</div>\n",
    "                <div class=\"animal\">parker</div>\n",
    "            </li>\n",
    "        </ul>\n",
    "        \n",
    "        <ul id=\"tim\">\n",
    "            <li class=\"tdawg\">\n",
    "                <span>yo im here</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <li>stuff</li>\n",
    "        <li>stuff2</li>\n",
    "        \n",
    "        <div id=\"cooldiv\">\n",
    "            <span class=\"dsi-rocks\">\n",
    "               YO!\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        \n",
    "        <waldo>Waldo</waldo>\n",
    "        <waldo>asdasd</waldo>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** We can use the asterisk special character '*' as an place holder for 'all possible'.\n",
    "\n",
    "```python\n",
    "# all elements where class='alpha'\n",
    "Selector(text=HTML).xpath('//*[@class=\"alpha\"]').extract()\n",
    "\n",
    "\n",
    "\n",
    "#returns\n",
    "\n",
    "[u'<div class=\"alpha\">Bill gates</div>',\n",
    " u'<div class=\"alpha\">Zuckerberg</div>']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find element 'waldo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Waldo', u'asdasd']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text contents of the element waldo\n",
    "Selector(text=HTML).xpath('/html/body/waldo/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find attribute(s) 'waldo'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<ul id=\"waldo\">\\n            <li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>\\n            <li class=\"waldo\">Height:  ???</li>\\n            <li class=\"waldo\">Weight:  ???</li>\\n            <li class=\"waldo\">Last Location:  ???</li>\\n            <li class=\"nerds\">\\n                <div class=\"alpha\">Bill gates</div>\\n                <div class=\"alpha\">Zuckerberg</div>\\n                <div class=\"beta\">Theil</div>\\n                <div class=\"animal\">parker</div>\\n            </li>\\n        </ul>',\n",
       " u'<li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>',\n",
       " u'<li class=\"waldo\">Height:  ???</li>',\n",
       " u'<li class=\"waldo\">Weight:  ???</li>',\n",
       " u'<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all attributes named waldo\n",
    "Selector(text=HTML).xpath('//*[@*=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<li class=\"waldo\">\\n                <span> yo Im not here</span>\\n            </li>',\n",
       " u'<li class=\"waldo\">Height:  ???</li>',\n",
       " u'<li class=\"waldo\">Weight:  ???</li>',\n",
       " u'<li class=\"waldo\">Last Location:  ???</li>']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of all class attributes named waldo\n",
    "Selector(text=HTML).xpath('//*[@class=\"waldo\"]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find text element Waldo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<waldo>Waldo</waldo>']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets everything around the text element waldo\n",
    "Selector(text=HTML).xpath(\"//*[text()='Waldo']\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "\n",
    "## Using Requests + Beautiful Soup to extract information from a webpage.\n",
    "\n",
    "---\n",
    "\n",
    "Beautiful Soup is a python library useful for pulling data out of HTML and XML files.  It works with many parsers, such as XPath and can be executed in an IDE, so it can be much easier to work with when first extracting information from html.\n",
    "\n",
    "Please make sure that the required packages are installed: \n",
    "\n",
    "```bash\n",
    "# beautiful soup:\n",
    "> conda install bs4 \n",
    "> conda install lxml\n",
    "\n",
    "# or if conda doesn't work\n",
    "> pip install bs4\n",
    "> pip install lxml\n",
    "```\n",
    "\n",
    "Lets find another posting for a sweet set of wheels on Craigslist (You will probably ave to update the URL to one that hasn't expired.):\n",
    "\n",
    "![](assets/craigslist.jpg)\n",
    "\n",
    "> *Note: you will need to update this to a current/working craigslist post.*\n",
    "\n",
    "https://washingtondc.craigslist.org/doc/cto/6178337288.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: fetch the content by URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n",
      "\n",
      "First part of HTML document fetched as string:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\">\n",
      "<head>\n",
      "<title>1983 Mercedes 380SL - cars &amp; trucks - by owner - vehicle automotive sale</title>\n",
      "    \t<link rel=\"canonical\" href=\"http://washingtondc.craigslist.org/doc/cto/6178337288.html\">\n",
      "\t<meta name=\"description\" content=\"A true Mercedes Benz classic that will only go up in value... This beautiful Grey 1983 well cared for Mercedes has a 3.8L V8 engine, 4 speed Automatic Transmission, PS, PB. Replaced original grey...\">\n",
      "\t<meta name=\"robots\" content=\"noar\n"
     ]
    }
   ],
   "source": [
    "# you will need the requests library in order to fully utilize bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# target web page\n",
    "url = \"https://washingtondc.craigslist.org/doc/cto/6178337288.html\"\n",
    "# establishing the connection to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "#Ex. 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found\n",
    "print 'Status Code: ',response.status_code\n",
    "\n",
    "# Pull HTML string out of requests and convert to a python string\n",
    "html = response.text\n",
    "\n",
    "# The first 500 characters of the content\n",
    "print \"\\nFirst part of HTML document fetched as string:\\n\"\n",
    "print html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More information on request status codes](http://www.restapitutorial.com/httpstatuscodes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Parse HTML document with Beautiful Soup\n",
    "\n",
    "This step allows us to access the elements of the document by XPATH expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soup queries are more like accessing information within a python object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** There are many ways to get the elements in a \"soup\" object\n",
    "\n",
    "Here are a few ways to select HMTL elements as \"objects\" within \"soup\" as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>1983 Mercedes 380SL - cars &amp; trucks - by owner - vehicle automotive sale</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular element\n",
    "soup.html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983 Mercedes 380SL - cars & trucks - by owner - vehicle automotive sale\n"
     ]
    }
   ],
   "source": [
    "# Just the text between elements\n",
    "print soup.html.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'CL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find single or multiple elements\n",
    "# First parameter\n",
    "element = soup.findAll(\"a\", {\"class\": \"header-logo\"})\n",
    "element[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'$18500'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_search = soup.findAll('span', {\"class\": \"price\"})\n",
    "price_search[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switching back to SF apartment listings\n",
    "response = requests.get(\"http://sfbay.craigslist.org/search/sfc/apa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "search_titles = soup.findAll(\"a\", {\"class\": \"hdrlnk\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data-id': '6178462035', 'href': '/sfc/apa/6178462035.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178462043', 'href': '/sfc/apa/6178462043.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178461647', 'href': '/sfc/apa/6178461647.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178461105', 'href': '/sfc/apa/6178461105.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178460712', 'href': '/sfc/apa/6178460712.html', 'class': ['result-title', 'hdrlnk']}\n"
     ]
    }
   ],
   "source": [
    "for link in search_titles[0:5]:\n",
    "    print link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### > **Check:** How do we know which parameters `findAll()` takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "### Practice: can you select the price of our junker?  \n",
    "\n",
    " - Use XPath Helper to get an idea of where the element is within the HTML document.\n",
    " - Try to select using the soup.html.body.something.something method.\n",
    " - Try using findAll() to find a concise element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "<a scrapy-spiders></a>\n",
    "## What is [Scrapy](http://scrapy.org/)?\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"*\n",
    "\n",
    "Below we will walkthrough the creation of a **spider** using scrapy. Spiders are automated processes that will crawl through a webpage or webpages and collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of jupyter notebook.\n",
    "\n",
    "<a id='scrapy-project'></a>\n",
    "### 1. Create a new Scrapy project\n",
    "\n",
    "In your terminal. `cd` into a directory you want to create your Crawler's folder.  I recommend the desktop for ease of access to the files inside we will need to edit.\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "**Should create output that looks like this:**\n",
    "<blockquote>\n",
    "```\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Overridden settings: {}\n",
    "New Scrapy project 'craigslist' created in:\n",
    "    /Users/davidyerrington/virtualenvs/data/scraping/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "```\n",
    "craigslist/\n",
    "    scrapy.cfg\n",
    "    craigslist/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files.  We will go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** the project configuration file\n",
    " * **`craigslist/`:** the project’s python module, you’ll later import your code from here.\n",
    " * **`craigslist/items.py`:** the project’s items file.\n",
    " * **`craigslist/pipelines.py`:** the project’s pipelines file.\n",
    " * **`craigslist/settings.py`:** the project’s settings file.\n",
    " * **`craigslist/spiders/`:** a directory where you’ll later put your spiders.\n",
    " \n",
    "Long story, but please add this line to your craigslist/settings.py file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='define-item'></a>\n",
    "### 2. Define an \"item\"\n",
    "\n",
    "Basically, when we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (for those who are familliar with ORM or relational database terms).  Don't worry if this is a foreign concept.  The main idea to understand is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='spider-crawl'></a>\n",
    "### 3. A spider that crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls pages and uses our item model to to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are python classes.  Let's write our first file, called `craigslist_spider.py` and put it in our `/spiders` directory:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"http://sfbay.craigslist.org/search/sfc/apa\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/craigslist/craigslist` directory:**\n",
    "\n",
    "```\n",
    "> scrapy crawl craigslist\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory.  It should be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate the workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='xpath-spider'></a>\n",
    "### 4. XPath + parsing with our spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "**We should let our spider know about the item model we made earlier.  In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, lets add a new import:**\n",
    "\n",
    "```python\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "\n",
    "> **Check:** Why won't it work otherwise?\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our parse method, to find some data from our Craigslist spider response, and map it to our item model, CraigslistItem:**\n",
    "\n",
    "\n",
    "```python\n",
    "def parse(self, response): # define parse function \n",
    "    items = [] # element for storing scraped info\n",
    "\thxs = Selector(response) # selector is a function that allows us to grab html from the response(target website)\n",
    "\tfor sel in hxs.xpath(\"//li[@class='result-row']/p\"): # as we're using xpath languange we need to specify\n",
    "                        # that the paragraphs we are trying to isolate are expressed via xpath\n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() #title text from the 'a' element \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() # href/url from the 'a' element \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]\n",
    "                # price from the result price class nested in a few span elements.\n",
    "        items.append(item)\n",
    "\treturn items # shows scraped information as terminal output\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='save-examine'></a>\n",
    "### Save and examine our scraped data\n",
    "\n",
    "By default, we can save our crawled data as csv.  To save our data, we just need to pass a few optional parameters to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "> scrapy crawl craigslist -o items.csv -t csv\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check our data when developing a spider to make sure it's close to what we want. \n",
    "\n",
    "> *Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's not working and fix bugs.*\n",
    "\n",
    "You should now have a file called '`items.csv`' in the directory you ran the `scrapy crawl` command from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addendum'></a>\n",
    "## Addendum: leveraging XPath to get more results\n",
    "\n",
    "---\n",
    "\n",
    "Generally, a workflow that is useful in this context is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, and from that derive your own XPath expressions based on the output.\n",
    "\n",
    "`text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples of `text()`**:\n",
    "<blockquote>\n",
    "```\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "//h1/text()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**Here are a few examples of attributes**:\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "```\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "```\n",
    "//div[@id='description']\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following links for more results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  Using the **`parse()`** method of our spider class, we only need to return another type of object.\n",
    "\n",
    "```python\n",
    "def parse(self, response):  \n",
    "\n",
    "    items = [] \n",
    "\thxs = Selector(response) \n",
    "    titles = hxs.xpath(\"//li[@class='result-row']/p\")\n",
    "    \n",
    "\tfor sel in titles:                     \n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]     \n",
    "        items.append(item)\n",
    "\treturn items \n",
    "\n",
    "    # Does the next page exist?  Let's get it!\n",
    "    next_page   = response.xpath(\"(//a[@class='button next']/@href)[1]\")\n",
    "\n",
    "    if next_page:\n",
    "        url = response.urljoin(next_page[0].extract())\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
